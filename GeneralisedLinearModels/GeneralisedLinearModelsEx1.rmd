---
title: "Generalised Linear Models Exercise 1"
author: Stephen Wotton (210138744)
geometry: "left=2cm,right=2cm,top=2cm,bottom=2cm"
output: pdf_document
---

```{r setup, include=TRUE}
#install packages
if(!(require(tidyverse))){install.packages('tidyverse')}

#load libraries
library(tidyverse)

#load source files
load("data/GLM.RData")
```
## Question 1
```{r include=TRUE}
digits <- 3
options(digits=digits)

# Logit is the default link but no harm being explict.
model.a <- glm(propn.dead ~ conc, family = binomial(link="logit"), weights=number, data=beetle)

summary(model.a)

model.a.predicted.dead <- model.a$fitted*beetle$number
model.a.predicted.dead

model.a.beta.0 <- round(model.a$coefficients[[1]], digits = digits)
model.a.beta.1 <- round(model.a$coefficients[[2]], digits=digits)
print(str_glue("Model A is: {model.a.beta.0} + {model.a.beta.1}x[i]"))
model.a.diff <-  beetle$dead - model.a.predicted.dead
model.a.diff.squared <- model.a.diff^2
model.a.error <- round(sum(model.a.diff.squared), digits = digits)

model.b <- glm(propn.dead ~ conc + I(conc^2), family = binomial(link="logit"), weights=number, data=beetle)

summary(model.b)

model.b.predicted.dead <- model.b$fitted*beetle$number
model.b.predicted.dead

model.b.beta.0 <- round(model.b$coefficients[[1]], digits = digits)
model.b.beta.1 <- round(model.b$coefficients[[2]], digits = digits)
model.b.beta.2 <- round(model.b$coefficients[[3]], digits = digits)
print(str_glue("Model B is: {model.b.beta.0} + {model.b.beta.1}x[i] + {model.b.beta.1}x[i]^2"))

model.b.diff <-  beetle$dead - model.b.predicted.dead
model.b.diff.squared <- model.b.diff^2
model.b.error <- round(sum(model.b.diff.squared), digits = digits)

print(str_glue("Model A error is: {model.a.error}, model B error is: {model.b.error}"))
```
Hence as we would expect with more parameters in the model we can achieve a better fit with the quadratic term of model B.

## Question 2

### A
```{r include=TRUE}
desired.vector.length <- 30
set.seed(3)
poission <- rpois(desired.vector.length, lambda = 10)
poission
```
### B
```{r include=TRUE}
normal.n <- rnorm(desired.vector.length, mean = 1, sd=1)
covariates.z <- rnorm(desired.vector.length, mean=0, sd=2)
normal.n
covariates.z
```
### C
```{r include=TRUE}
beta.zero =  rep(c(-1), times=desired.vector.length)
phi.input.values <- beta.zero  + normal.n + 0.4*covariates.z
phi.input.values
prob.pi <- pnorm(phi.input.values)
prob.pi
```
### D
```{r include=TRUE}

binomCreate <- function(i) {
  n <- 1
  s <- poission[i]
  p <- prob.pi[i]

  rbinom(n=n, size=s, prob=p)
}

interate.vec <- 1:desired.vector.length

bin.y <- sapply(interate.vec, binomCreate)
bin.y
```
## Question 3
Note that the exercise sheet does not say question three but the list going back to (a) suggests that it is.

#### A
```{r include=TRUE}
log.reg.pro <- glm( prob.pi ~ normal.n + covariates.z, family = binomial(link="probit"))
summary(log.reg.pro)

log.reg.pro.fitted <- log.reg.pro$fitted
log.reg.pro.fitted.in.requested.range <- log.reg.pro.fitted[1:10]
log.reg.pro.fitted.in.requested.range
print(str_glue("To calculate y5 by hand we need variables: x5 {round(normal.n[5], digits=4)}, z5 {round(covariates.z[5], digits=4)}"))
```
Therefore $y_5 = \phi(-1 + x_5 + 0.4*z_5) = \phi(-1 +  0.6895 +  0.4*0.9342) = \phi(0.06318) = 0.525189$
This was worked out on calculator, note when rounded off to same number of significant places the value is the same as R, namely $y_5= 0.5252$.



#### B
```{r include=TRUE}
log.reg.logit <- glm( prob.pi ~ normal.n + covariates.z, family = binomial(link="logit"))
summary(log.reg.logit)

log.reg.logit.fitted <- log.reg.logit$fitted
log.reg.logit.fitted.in.requested.range <- log.reg.logit.fitted[1:10]
log.reg.logit.fitted.in.requested.range
print(str_glue("To calculate y5 by hand we need variables: x5 {round(normal.n[5], digits=6)}, z5 {round(covariates.z[5], digits=6)}"))
```
Therefore $u_5 = h(x_5) = h(-1.694 + 1.694*x_5 + 0.687*z_5) = h(-1.694 + 1.694*0.689537 + 0.687*0.934195) = h(0.1158676) = exp(0.1158676)/(1 + exp(0.1158676) = 0.5289193$
This was worked out on calculator, the number is approximately correct however there must be rounding issues somewhere as R gets 0.5291 and I get 0.52892 despite trying to work to greater precision!!  Apologies for this.

#### Commentry
The residual deviance (a measure of how good the predictor is by comparing error) was:

  * probit link model was: `Residual deviance: 6.5677e-16  on 27  degrees of freedom`
  * logit link model was `Residual deviance:  0.020006  on 27  degrees of freedom`

This makes the probit link model a better model.

Comparing the null deviance we had:

  * probit link model was: `Null deviance: 1.4322e+01  on 29  degrees of freedom`
  * logit link model was `Null deviance: 14.321802  on 29  degrees of freedom`

Therefore the probit's models intercept term better explains the data.  However both models are good predictors as the residual deviance shows.